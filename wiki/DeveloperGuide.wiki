#summary Detailed guide for the developers to understand the design and implementation of DataPath

= Introduction =
 
DataPath is a data processing engine designed from ground up to fully utilize modern hardware for large analytical queries. With DataPath, it is possible to process in minutes 1TB dataset and in minutes to hours 10TB datasets with 20,000-40,000$ hardware investment.

= Design Philosophy = 

When designing DataPath, we made it a point to start with a clean slate. We strived to avoid reusing either code or ideas from traditional database systems. This allowed us to reconsider the design of a data processin engine from ground up, to try to make the best use of modern architectures without any attempt to salvage any existing effort. 

The major design consideratins for DataPath were:

  # *Truly multi-threaded execution engine*: The engine should fully utilize multi-core systems even if the query involves few operators. Take for example a query like Q1 in TPC-H. It only involves a scan of `lineitem`, a selection on date and an group by with aggregate. Any implementation in a relational engine would use 2 or 3 operators (depending on whether the scan is performed through an index). To take advantage of a system with 16-64 cores, both the selection and the aggregate should be multi-threaded and make use of many cores.

  # *Data driven*: The engine should focus on the data and data movement rather than the computation. The main reason for this is the realization that memory bandwidth is the most precious resource. A direct consequence of this is the fact that, once the data is accessed, as much computation as possible should be performed. When running multiple queries, the system should perform work on behalf of all queries that could use the data, thus saving memory bandwidth. This is a generalization of the _shared scan_ ideas in databases; not only the data from the disk should be shared but the access of the data in memory.

  # *No/little tuning*: Database tuning is a truly complicated endeavor and a large industry as a result of this. It is not uncommon for a skilled DBA to speed up query processing by a factor of 10X or more by careful tuning of the database parameters. While possible, such tuning is too expensive for most users that cannot afford the high cost of such a skill. Even if it is possible to get reasonable performance at 1TB with a 20,000$ system, the cost of tuning  and of the software would significantly raise the cost. This is one of the main reasons map-reduce solutions are sometimes preferred. DataPath strives to require very little tuning, most of which is automatable.  For the most part, the system should figure out how to set itself up to run fast.

  # *High-performance*: Any system strives to achieve high-performance, making this a default goal. In DataPath, we identified a concrete way to achieve this for large queries: on-the-fly code generation. DataPath will generate _custom_ C++ code to execute the query workload, code that is devoid of data-structures and any interpretation. Effectively, all actions are hard-wired and the code is similar to expertly written custom code. With a cost of 2-5s for generation and compilation, the code executed by DataPath can be 10-100X faster than code of other database or data processing systems. 